use std::str::FromStr;
use std::fmt::Debug;

#[derive(Debug, PartialEq)]
pub enum Token {
    EndLine,               Identifier(String),
    IntLiteral(i64),       FloatLiteral(f64),
    StringLiteral(String), True,
    False,                 If,
    Elif,                  Else,
    While,                 Return,
    Var,                   BracketOpen,
    BracketClose,          SquareBracketOpen,
    SquareBracketClose,    CurlyBracketOpen,
    CurlyBracketClose,     Comma,
    Assignment,            Add,
    Subtract,              Multiply,
    Divide,                Mod,
    BWOr,                  BWAnd,                 
    BWXor,                 BWNot,
    Not,                   And,                   
    Or,                    Equality,              
    Inequality,            LessThan,              
    GreaterThan,           LessThanOrEqual,       
    GreaterThanOrEqual,    Increment,             
    Decrement,             Nothing
}

type TokenResult = Result<(Token, usize), String>;

fn make_error(msg: &str, subject: &str) -> TokenResult {
    Err(format!("{}: '{}'", msg, subject))
}

fn get_nth(string: &str, index: usize) -> char {
    let res = string.chars().nth(index);
    match res {
        Some(c) => c,
        None    => '\0'
    }
}

fn get_operator_token(string: &str) -> Option<Token> {
    Some(match string {
        "("  => Token::BracketOpen,        ")"  => Token::BracketClose,
        "["  => Token::SquareBracketOpen,  "]"  => Token::SquareBracketClose,
        "{"  => Token::CurlyBracketOpen,   "}"  => Token::CurlyBracketClose,
        ","  => Token::Comma,              "="  => Token::Assignment,
        "+"  => Token::Add,                "-"  => Token::Subtract,
        "*"  => Token::Multiply,           "/"  => Token::Divide,
        "|"  => Token::BWOr,               "&"  => Token::BWAnd,
        "^"  => Token::BWXor,              "!"  => Token::BWNot,
        ">"  => Token::GreaterThan,        "<"  => Token::LessThan,
        "==" => Token::Equality,           "!=" => Token::Inequality,
        ">=" => Token::GreaterThanOrEqual, "<=" => Token::LessThanOrEqual,
        "+=" => Token::Increment,          "-=" => Token::Decrement,
        ";"  => Token::EndLine,            "%"  => Token::Mod,
        _    => return None
    })
}

fn get_word_token(string: &str) -> Option<Token> {
    Some(match string {
        "if"      => Token::If,
        "elif"    => Token::Elif,
        "else"    => Token::Else,
        "while"   => Token::While,
        "return"  => Token::Return,
        "var"     => Token::Var,
        "true"    => Token::True,
        "false"   => Token::False,
        "and"     => Token::And,
        "or"      => Token::Or,
        "not"     => Token::Not,
        "nothing" => Token::Nothing,
        _         => Token::Identifier(string.to_string())
    })
}

fn get_number_token(string: &str) -> Option<Token> {
    // Parse a string to type T and pass it to make() for a token
    fn make_token<T: FromStr>(
        s: &str, 
        make: fn(T) -> Token
    ) -> Option<Token> {
        let parsed = s.parse::<T>();
        match parsed {
            Ok(res) => Some(make(res)),
            Err(_)  => None
        }
    }

    if string.contains(".") {
        return make_token::<f64>(string, |x| Token::FloatLiteral(x));
    } else {
        return make_token::<i64>(string, |x| Token::IntLiteral(x));
    }
}

fn tokenise_generic(
    text: &str, 
    start_index: usize, 
    get_token: fn(&str) -> Option<Token>,
    predicate: fn(char) -> bool
) -> TokenResult {
    let mut end_index = start_index + 1;
    let mut slice: &str;

    loop {
        slice = &text[start_index .. end_index];

        let end = get_nth(text, end_index);
        if !predicate(end) {
            let token = get_token(slice);
            return match token {
                None    => make_error("Invalid token", slice),
                Some(t) => Ok((t, end_index))
            }
        }

        end_index += 1;
    }    
}

fn tokenise_word(text: &str, start_index: usize) -> TokenResult {
    tokenise_generic(text, start_index, get_word_token, 
        |c| c.is_ascii_alphanumeric() || c == '_')
}

fn tokenise_number(text: &str, start_index: usize) -> TokenResult {
    tokenise_generic(text, start_index, get_number_token, 
        |c| c.is_ascii_alphanumeric() || c == '_' || c == '.')
}

fn tokenise_string(text: &str, start_index: usize) -> TokenResult {
    let mut end_index = start_index + 1;

    loop {
        if end_index == text.len() {
            let slice = &text[start_index .. end_index];
            return make_error("Unterminated string literal", slice);
        }

        let end = get_nth(text, end_index);

        if end == '\"' {
            let slice = &text[start_index+1 .. end_index];
            let token = Token::StringLiteral(slice.to_string());
            return Ok((token, end_index+1))
        }
        end_index += 1;
    }
}

fn tokenise_operator(text: &str, start_index: usize) -> TokenResult {
    let mut end_index = start_index + 1;
    let mut token_end_index = end_index;
    let mut token: Option<Token> = None;

    loop {
        let slice = &text[start_index .. end_index];
        let current = get_operator_token(slice);
        if current.is_some() {
            token = current;
            token_end_index = end_index;
        }

        let end = get_nth(text, end_index);
        if !end.is_ascii_punctuation() {
            return match token {
                None    => make_error("Invalid operator", slice),
                Some(t) => Ok((t, token_end_index))
            };
        }

        end_index += 1;
    }
}

fn eat_comment(text: &str, start_index: usize) -> usize {
    let mut end_index = start_index + 1;
    while end_index < text.len() {
        let c = get_nth(text, end_index);
        if c == '\n' {
            
        }
    }
    end_index
}

pub fn tokenise(text: &str) -> Result<Vec<Token>, String> {
    let mut tokens: Vec<Token> = Vec::new();
    let mut start_index = 0;
    
    while start_index < text.len() {
        let first = get_nth(text, start_index);
        
        let res = 
            if first.is_ascii_whitespace() {
                start_index += 1;
                continue;
            } else if first == '"' { 
                tokenise_string(text, start_index)   
            } else if first.is_ascii_alphabetic() || first == '_' { 
                tokenise_word(text, start_index)     
            } else if first.is_ascii_punctuation() { 
                tokenise_operator(text, start_index) 
            } else if first.is_ascii_digit() { 
                tokenise_number(text, start_index)   
            } else { 
                make_error("Invalid character", &first.to_string())     
            };

        match res {
            Err(msg) => return Err(msg),
            Ok((token, end)) => {
                tokens.push(token);
                start_index = end;
            }
        }
    }

    Ok(tokens)
}

//
// Tests
// 

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn all_tokens_correct() {
        let tokens = "
            ;          identifier 123123 123.123 
            \"string\" true       false  if 
            elif       else       while  return
            var        (          )      [
            ]          {          }      ,
            =          +          -      *
            /          %          |      &      
            ^          !          not    and    
            or         ==         !=     <      
            >          <=         >=     +=     
            -=         nothing
        ";
        let toks   = tokenise(tokens).unwrap();
        let string = "string".to_string();
        let id     = "identifier".to_string();
        let should_be = vec![
            Token::EndLine,               Token::Identifier(id),
            Token::IntLiteral(123123),    Token::FloatLiteral(123.123),
            Token::StringLiteral(string), Token::True,
            Token::False,                 Token::If,
            Token::Elif,                  Token::Else,
            Token::While,                 Token::Return,
            Token::Var,                   Token::BracketOpen,
            Token::BracketClose,          Token::SquareBracketOpen,
            Token::SquareBracketClose,    Token::CurlyBracketOpen,
            Token::CurlyBracketClose,     Token::Comma,
            Token::Assignment,            Token::Add,
            Token::Subtract,              Token::Multiply,
            Token::Divide,                Token::Mod,
            Token::BWOr,                  Token::BWAnd,                 
            Token::BWXor,                 Token::BWNot,                 
            Token::Not,                   Token::And,                   
            Token::Or,                    Token::Equality,              Token::Inequality,            Token::LessThan,              Token::GreaterThan,           Token::LessThanOrEqual,       Token::GreaterThanOrEqual,    Token::Increment,             Token::Decrement,             Token::Nothing
        ];

        assert_eq!(toks, should_be);
    }

    #[test]
    fn invalid_int_literal() {
        let error = tokenise("variable = 1abc").err().unwrap();
        assert_eq!(error, "Invalid token: '1abc'")
    }

    #[test]
    fn invalid_float_literal() {
        let error = tokenise("variable = 1.2.1").err().unwrap();
        assert_eq!(error, "Invalid token: '1.2.1'")
    }

    #[test]
    fn invalid_operator() {
        let error = tokenise("?=").err().unwrap();
        assert_eq!(error, "Invalid operator: '?='")
    }

    #[test]
    fn unescaped_string_literal() {
        let error = tokenise("\"string").err().unwrap();
        assert_eq!(error, "Unterminated string literal: '\"string'")
    }

    #[test]
    fn invalid_char() {
        let error = tokenise("\0").err().unwrap();
        assert_eq!(error, "Invalid character: '\u{0}'")
    }
}